

\end{document}\documentclass[12pt]{article}
\usepackage{design_ASC}
\usepackage{graphicx}
\graphicspath{ {./images/} }
% \usepackage[LGR]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[greek,english]{babel}
\usepackage{alphabeta}
\usepackage{hyperref}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand\taba[1][3cm]{\hspace*{#1}}
\usepackage{amsmath}
\DeclareUnicodeCharacter{2212}{-}

\setlength\parindent{0pt} 

%% -----------------------------
%% TITLE
%% -----------------------------
\title{Problem Set 2} %% Assignment Title

\author{Τρίμας Χρήστος\\ %% Student name
AM:2016030054 \\ Στατιστική Μοντελοποιήση και Αναγνώριση Προτύπων\\ %% Code and course name
\textsc{Πολυτεχνείο Κρήτης}
}

\date{\today} %% Change "\today" by another date manually
%% -----------------------------
%% -----------------------------

%% %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\setlength{\droptitle}{-5em}    
%% %%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

% --------------------------
% Start here
% --------------------------

% %%%%%%%%%%%%%%%%%%%
\section*{Ερώτηση 1}
% %%%%%%%%%%%%%%%%%%%
{\bfseries Λογιστική Παλινδρόμηση, Αναλυτική εύρεση κλίσης:}\\ \\
Στην 1η άσκηση της 2ης εργασίας, θα ασχοληθούμε με την λογιστική παλινδρόμηση. Πρίν όμως γίνει μαθηματική ερμηνεία του αλγορίθμου παλινδρόμησης, θα κάνω visualize το dataset στο οποίο θα δουλέψω πάνω.\\\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/logistic_1.png}
    \caption{Dataset}
    \label{fig:my_label}
\end{figure}
 Συγκεκριμένα, τα δεδομένα αφορούν βαθμούς εξετάσεων μαθημάτων φοιτητών. Σκοπός είναι να γίνει μια καλή πρόβλεψη με παλινδρόμηση για το εάν ο φοιτητής θα γίνει δεκτός ή όχι σε ένα πανεπιστήμιο με βάση τους βαθμούς του στις δυο αυτές εξετάσεις(άξονες του figure).\\\\
 Για να τρέξει με επιτυχία ο κώδικας μας, χρειάστηκε να υλοποιηθούν μεταξύ άλλων και κάποιες άλλες συναρτήσεις. Συγκεκριμένα, η sigmoid συνάρτηση δέχεται σαν είσοδο έναν πίνακα z, και εφαρμόζει σε αυτό τον πίνακα τον ακόλουθο μετασχηματισμό: \[f(z) = \frac{1}{1+e^{-z}} \]\\\\
 Η συνάρτηση f, είναι η λογιστική συνάρτηση, η οποιά για μεγάλες θετικές τιμές του x, πρέπει να είναι κοντά στο 1, ενώ για μεγάλες αρνητικές τιμές κοντά στο μηδέν. Σε περίπτωση που x=0, τότε πρέπει να μας δίνει ακριβώς \(\frac{1}{2}.\)\\\\
 Επιπλέον, υλοποιήθηκε η συνάρτηση costFunction, η οποία επιστρέφει το κόστος J(θ) και την κλίση gradient αυτής, όπως ορίζεται παρκάτω:\\\\
 \[J(θ) = \frac{1}{m} \displaystyle\sum_{i=1}^{m} [-y^{(i)}log(h_θ(x^{(i)}))-(1-y^{(i)})log(1-h_θ(x^{(i)}))]\]
 Και το gradient του κόστους ορίζεται ως εξής:\\\\
\[\frac{\partial J(θ)}{\partial θ_j} = \frac{1}{m}\displaystyle\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})x^{(i)}_j \]
 Παρατηρείται, ότι αν και η εξίσωση του gradient είναι πανομοιότυπη με αυτή της γραμμικής παλινδρόμησης, λόγο του διαφορετικού τρόπου ορισμού της υπόθεσης της παλινδρόμησης \(h_θ(x) = g(θ^{Τ}x)\), έχουμε λογιστική παλινδρόμηση και όχι γραμμική.\\\\
 Μπορούμε με ασφάλεια να συμπεράνουμε ότι μέχρι αύτο το σημέιο τα πάντα τρέχουν ομαλά στον κώδικα μας, καθώς για θ=0, τόσο το κόστος, όσο και η κλίση έχουν τα επιθυμητά αποτελέσματα.\\\\
 Τέλος,για να γίνει αξιολόγηση του μοντέλου, υλοποιήθηκε η συνάρτηση predict, η οποία παρήγαγε 1 ή 0 προβλέψεις, δοσμένου κάποιου συνολού δεδομένων και ενός διανύσματος θ.Για έναν μαθητή με σκορ 45,85 η πιθανοτητα να περασει ειναι σχεδον 0.77. \\\\
 Τα training data με το σύνορο απόφασης φαίνονται στην εικόνα:
 \begin{figure}[!h]
     \centering
     \includegraphics[width=5cm]{images/logistic_1_1.png}
     \caption{Training data with boundary}
     \label{fig:my_label}
 \end{figure}
 \FloatBarrier
\\ Με μια ματιά, μπορεί κανείς να συμπεράνει ότι το αποτέλεσμα είναι αρκετά καλό, και τα δεδομένα τα οποία χάνονται στην πρόβλεψη σημαντικά λίγα, άλλωστε το accuracy είναι 0.89/1. Ωστόσο, αυτό συμβαίνει, διότι τα δεδομένα είναι και σχεδόν γραμμικά διαχωρίσιμα. Στην αμέσως επόμενη άσκηση θα μελετήσουμε την εφαρμογή της λογιστικής παλινδρόμησης σε ένα dataset με μη γραμμικά διαχωρίσιμα δεδομένα.\\\\
 
 \section*{Ερώτηση 2}
{\bfseries Λογιστική Παλινδρόμηση με Ομαλοποίηση:}\\\\
 Στο συγκεκριμένο πρόβλημα, εφαρμόστηκε η ομαλοποιημένη λογιστική παλινδρόμηση σε ένα σετ από δεδομένα μιας εταιρείας παραγωγής τσιπ. Συγκεκριμένα, θέλουμε να αυτοματοποιήσουμε την διαδιακασία του ποιοτικού ελέγχου, και έχοντας τα δεδομένα από διαφορετικές δοκιμές θέλουμε να κατασκευάσουμε έναν regressor με την βέλτιστη δυνατή ακρίβεια.\\\\
 Οπως και στην άσκηση 1, ξεκινάμε με την οπτικοποιήση των δεδομένων.\\\\
 \begin{figure}[!h]
     \centering
     \includegraphics[width=7cm]{images/exe_2_orig.png}
     \caption{Original Dataset}
     \label{fig:my_label}
 \end{figure}
Παρατηρείται, ότι ο διαχωρισμός των δεδομένων δεν είναι δυνατόν να πραγματοποιηθεί, από μια γραμμή. Για αυτό το λόγο, η απευθείας χρήση της λογιστικής παλινδρόμησης δεν είναι καλή επιλογή για λύση του προβλήματος μας.\\\\
Ένας τρόπος για να γίνει αποτελεσματικά η παλινδρόμηση, είναι να προσθέσουμε επιπλέον χαρακτηριστικά στα δεδομένα μας ή αλλιώς να απεικονίσουμε το dataset σε χώρο υψηλώτερων διαστάσεων, όπου ο διαχωρισμός είναι εφικτός. Η διαδικασία αυτή, επιτυγχάνεται με την συνάρτηση mapFeature, η οποία κάνει map τα χαρακτηριστικά στους πολυωνυμικούς όρους της κάθε κλάσης, μέχρι και την 6η δύναμη.\\\\
\[mapFeature(x) = \begin{bmatrix}
                    1\\
                    x_1\\
                    x_2\\
                    x_1^2\\
                    x_2^2\\
                    \vdots\\
                    x_1x_2^5\\
                    x_2^6\\
                    \end{bmatrix}\]
\\Αποτέλεσα αυτού του μετασχηματισμού, είναι η δημιουργία ενός 28x1 διανύσματος. Η παλινδρόμηση με λογιστικό παλινδρομητή, θα δημιουργήσει ένα αρκετά πιο σύνθετο όριο διαχωρισμού, όπως θα δούμε και παρακάτω.\\\\
Σε ότι αφορά την συνάρτηση κόστους και την κλίση της, αυτά δίνονται από τις σχέσεις:\\\\
\[J(θ) = \frac{1}{m} \displaystyle\sum_{i=1}^{m} (-y^{(i)}ln(h_θ(x^{(i)}))-(1-y^{(i)})ln(1-h_θ(x^{(i)})) + \frac{λ}{2m} \displaystyle\sum_{j=1}^{n} θ_j^{2}\]
 Και το gradient από την σχέση:\\\\
 \[\nabla J(θ) = \frac{1}{m} \displaystyle\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{λ}{m}θ_j\]
 \\\\Με χρήση της αρχικής τιμής θ, το αρχικό κόστος είναι 0.693147
 \\\\Παρακάτω, απεικονίζεται το σύνορο απόφασης για διαφορετικές τιμές του λ = {0,1,10,100}.\\\\
 \begin{figure}[!h]
     \centering
     \includegraphics[width=6cm]{images/lam0.png}
     \caption{Decision Boundary, λ=0}
     \label{fig:my_label}
 \end{figure}
 \\\\
\\\\\\\\ Με την χαμηλότερη τιμή του λ=0, ο regressor πετυχαίνει το καλύτερο accuracy '88.983051', ωστόσο, το όριο απόφασης είναι όπως περιμέναμε αρκετά πολύπλοκο. Το φαινόμενο αυτό ονομάζεται over-fitting of the data. Αυτό, δεν είναι καλό σύνορο απόφασης, καθώς ένα σημείο πχ στο (0.6,0), το δέχεται σαν σημείο με y=0, ενώ θα έπρεπε να ανήκει στην άλλη κλάση.\\\\
\FloatBarrier

 
 
 \begin{figure}[!h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=5cm]{images/lam1.png}
  \caption{Decision Boundary, λ=1}
  \label{fig:my_label}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=5cm]{images/lam10.png}
  \caption{Decision Boundary, λ=10}
  \label{fig:my_label}
\end{minipage}
\end{figure}
\FloatBarrier
\\\\Για λ=1, το σύνορο απόφασης, γίνεται όλο και πιο απλό στο σχεδιασμό του, ωστόσο η ακρίβεια πέφτει με την αύξηση του λ. Συγκεκριμένα για λ=1 και λ=10, δεν συναντάμε φαινόμενο over-fitting ή under-fitting(οπως θα δούμε για μεγάλο λ, πχ λ=100). Σε κάθε περίπτωση, το accuracy για λ=1 είναι '83.050847' και για λ=10, '74.576271'\\\\

Τέλος, όπως υπόθηκε και προηγουμένως, για λ=100 έχουμε και την χειρότερη απόδοση '61.016949', και φαινόμενο under-fitting.\\\\

\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/lam100.png}
    \caption{Decision Boundary, λ=100}
    \label{fig:my_label}
\end{figure}

\section*{Ερώτηση 4}\\\\
{\bfseries Εκτίμηση Παραμέτρων και Ταξινόμηση :}\\ \\

Στην άσκηση 4 θα πειραματιστούμε με τους εκτιμητές Maximum Likelihood, και συγκεκριμένα χρησιμοποιήσουμε την κατανομή Bernoulli για να μοντεολοποιήσουμε το πρόβλημα μας.\\\\
Δεδομένου ενός dataset που περιλαμβάνει εικόνες από χειρόγραφα ψηφία, θέλουμε να κατασκευάσουμε έναν εκτιμητή, ο οποίος θα κάνει σωστή ταξινόμηση των ψηφίων, δηλαδή όταν σαν είσοδο έχουμε ένα χειρόγραφο 0, ο classifier μας, θα προβλέπει ποιο ψηφίο είναι αυτό, χρησιμοποιώντας την MLE πιθανότητα του ψηφίου. Με πολύ απλά λόγια, θα βρίσκει ποιο ψηφίο έχει την μεγαλύτερη πιθανότητα από τα άλλα ψηφία, και θα επιλέγει αυτό.\\\\

\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/9.png}
    \caption{Χειρόγραφο 9}
    \label{fig:my_label}
\end{figure}

Για το ερώτημα α δείτε την εικόνα στον φάκελο.\\\\

1) Τελικά, καταλήγουμε ότι: \[ MLE:\tab p = \frac{1}{n}\displaystyle\sum_{\forall i} x^{y^{i}}\]

2) Στη συνέχεια, υπολογίζουμε με τον τύπο παραπάνω τις πιθανότητες, χρησιμοποιώντας τα δεδομένα εκπαίδευσης train0,...,train9.Επειδή, χρησιμοποιείται η λογαριθμική μορφή της πιθανοτικής εκτίμησης Bernoulli, όσες τιμές p(0,...,9) είναι μηδέν, τις θέτουμε σε μια τιμή πάρα πολύ μικρή και στην συνέχεια υπολογίζουμε το logL(p), για κάθε ψηφίο, για κάθε δείγμα του ψηφίου.\\\\
Τέλος, για να οπτικοποιήσουμε το αποτέλεσμα μας, πολαπλασιάζουμε ότα τα δείγματα κάθε train set, με την logL(p) του και παίρνουμε ένα διάνυσμα 784x1, το οποίο κάνουμε reshape, με αποτέλεσμα να λάβουμε μια εικόνα, της οποίας τα pixel αντιπροσωπεύουν την πιθανότητα \(p^{y^{i}}\) της κατανομής Bernoulli.

\begin{figure}[!h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=5cm]{images/pro0.png}
  \caption{Digit with probability pixels}
  \label{fig:my_label}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=5cm]{images/pro0_pin.png}
  \caption{Απόσπασμα από τον πίνακα οπτικοποιήσης}
  \label{fig:my_label}
\end{minipage}
\end{figure}
\\\\
3) Για το 3ο σκέλος της άσκησης, θέλαμε να τεστάρουμε την απόδοση του Naive Bayes Classifier μας, δοκιμάζοντας τον σε ένα σύνολο από δεδομένα δοκιμής(test0,...test9). Κάθε τέτοιο σέτ, περιλαμβάνει 500 εικόνες, 28x28 πίξελ η κάθε εικόνα.\\\\
Το ερώτημα σε αυτό το κομμάτι είναι, πως θα αξιολογήσουμε τον classifier μας ? Η απάντηση είναι αρκετά απλή. Καθώς έχουμε γνώση τι ψηφία έχει το κάθε test σετ, θα τα δώσουμε στον classifier και θα περιμένουμε να επιλέξει το σωστό. Πχ αν του δώσουμε εικόνα από το train0, θα περιμένουμε να μας δείξει ότι το στοιχείο μηδέν έχει την μεγαλύτερη πιθανότητα από τα υπόλοιπα ψηφία.\\\\
Αυτή η διαδικασία θα γίνει για κάθε σετ εικόνων δοκιμής, και στη συνέχεια θα αξιολογήσουμε την απόδοση μέσω των εξής metrics:\\\\
\taba \tab \tab 1) Accuracy.
\tab 2) Confusion Matrix.\\\\

Τα διανύσματα x0,...,x9, επιστρέφουν την θέση, των ψηφίων που προβλέπει ο ταξινομητής μας. Καθώς το matlab ξεκινάει να μετράει τις τιμές από το 1, το 1 θα αντιστοιχεί στο ψηφίο 0, το 2 στο 1 κλπ. Άρα, στο διάνυσμα x0, περιμένουμε οι τιμές να είναι 1. Ωστόσο, λόγο λάθος ταξινόμησης των ψηφίων, παρατηρούνται κάποιες αστοχίες.\\\\
Συγκεκριμένα, στον ακόλουθο πίνακα φαίνεται η ακρίβεια κάθε ταξινομητή για το δικό του MLE:\\\\

\[AccuracyMatrix = \bordermatrix{digit & accuracy \cr\\[0.3em]
                                    0 &  0.8820 \cr\\[0.3em]
                                    1 & 0.9460 \cr\\[0.3em]
                                    2 & 0.7480 \cr\\[0.3em]
                                    3 & 0.8300\cr\\[0.3em]
                                    4 & 0.7520 \cr\\[0.3em]
                                    5 & 0.6900 \cr\\[0.3em]
                                    6 & 0.8240 \cr\\[0.3em]
                                    7 & 0.7880 \cr\\[0.3em]
                                    8 & 0.6960 \cr\\[0.3em]
                                    9 & 0.8180 \cr}\]
                                    
Τέλος, υπολογίστηκαν και οι πίνακες confusion(δες εκφώνηση για τον ορισμό τους) κάθε ψηφίου. Παρακάτω απεικονίζονται για διευκόλυνση έτσι ώστε να μην χρειαστεί να διαβαστούν από το matlab.CM είναι ο conf matrix κάθε ψηφίου.\\\\


                
\[CM0 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            1 & 0 \cr\\[0.3em]
                            2 &  1\cr\\[0.3em]
                            3 &  0\cr\\[0.3em]
                            4 &  2\cr\\[0.3em]
                            5 &  30\cr\\[0.3em]
                            6 &  13\cr\\[0.3em]
                            7 &  0\cr\\[0.3em]
                            8 &  12\cr\\[0.3em]
                            9 &  1\cr}\]
\\\\    
\[CM1 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 &  0\cr\\[0.3em]
                            2 &  2\cr\\[0.3em]
                            3 &  3\cr\\[0.3em]
                            4 &  0\cr\\[0.3em]
                            5 &  13\cr\\[0.3em]
                            6 &  4\cr\\[0.3em]
                            7 &  0\cr\\[0.3em]
                            8 &  5\cr\\[0.3em]
                            9 &  0\cr}\]
\\\\
\[CM2 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 & 8 \cr\\[0.3em]
                            1 &  12\cr\\[0.3em]
                            3 &  36\cr\\[0.3em]
                            4 &  6\cr\\[0.3em]
                            5 &  2\cr\\[0.3em]
                            6 &  11\cr\\[0.3em]
                            7 &  14\cr\\[0.3em]
                            8 &  33\cr\\[0.3em]
                            9 &  4\cr}\]
\\\\
\[CM3 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 & 1 \cr\\[0.3em]
                            1 &  10\cr\\[0.3em]
                            2 &  5\cr\\[0.3em]
                            4 &  4\cr\\[0.3em]
                            5 &  23\cr\\[0.3em]
                            6 &  6\cr\\[0.3em]
                            7 &  13\cr\\[0.3em]
                            8 &  10\cr\\[0.3em]
                            9 &  13\cr}\]
\\\\
\[CM4 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 & 1 \cr\\[0.3em]
                            1 & 1\cr\\[0.3em]
                            2 &  3\cr\\[0.3em]
                            3 &  0\cr\\[0.3em]
                            5 &  4\cr\\[0.3em]
                            6 &  16\cr\\[0.3em]
                            7 &  2\cr\\[0.3em]
                            8 &  5\cr\\[0.3em]
                            9 &  92\cr}\]
\\\\
\[CM5 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 & 15\cr\\[0.3em]
                            1 &  2\cr\\[0.3em]
                            2 &  2\cr\\[0.3em]
                            3 &  67\cr\\[0.3em]
                            4 &  19\cr\\[0.3em]
                            6 &  7\cr\\[0.3em]
                            7 &  6\cr\\[0.3em]
                            8 &  14\cr\\[0.3em]
                            9 &  23\cr}\]
\\\\
\[CM6 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 &  7\cr\\[0.3em]
                            1 &  10\cr\\[0.3em]
                            2 &  24\cr\\[0.3em]
                            3 &  0\cr\\[0.3em]
                            4 &  11\cr\\[0.3em]
                            5 & 33\cr\\[0.3em]
                            7 &  0\cr\\[0.3em]
                            8 &  3\cr\\[0.3em]
                            9 &  0\cr}\]
\\\\
\[CM7 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 & 1 \cr\\[0.3em]
                            1 &  21\cr\\[0.3em]
                            2 & 9 \cr\\[0.3em]
                            3 &  3\cr\\[0.3em]
                            4 &  17\cr\\[0.3em]
                            5 &  0\cr\\[0.3em]
                            6 &  0\cr\\[0.3em]
                            8 &  10\cr\\[0.3em]
                            9 &  45\cr}\]
\\\\
\[CM8 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 & 6 \cr\\[0.3em]
                            1 & 17 \cr\\[0.3em]
                            2 & 11\cr\\[0.3em]
                            3 &  49\cr\\[0.3em]
                            4 &  11\cr\\[0.3em]
                            5 &  18\cr\\[0.3em]
                            6 &  2\cr\\[0.3em]
                            7 &  3\cr\\[0.3em]
                            9 &  35\cr}\]
\\\\
\[CM9 =  \bordermatrix{wrong digit & numOfMissClassification \cr \\[0.3em]
                            0 & 4 \cr\\[0.3em]
                            1 &  8\cr\\[0.3em]
                            2 &  4\cr\\[0.3em]
                            3 &  8\cr\\[0.3em]
                            4 &  49\cr\\[0.3em]
                            5 &  7\cr\\[0.3em]
                            6 &  0\cr\\[0.3em]
                            7 &  6\cr\\[0.3em]
                            8 &  5\cr}\]
\\\\



\section*{Ερώτηση 5}
{\bfseries Bayesian εκτίμηση παραμέτρου :}\\ \\
Σε αυτή την άσκηση μελετήθηκε η Bayesian εκτίμηση της μ, γνωρίζοντας ότι η συνάρτηση πυκνότητας πιθανότητας των μετρήσεων του σετ δεδομένων ακολουθεί γκαουσιανή κατανομή, και συγκεκριμένα: \(p(x)\sim N(μ,σ^{2})\), και γνωστή τυπική αποκλιση σ=1.25. Υποθέτοντας ότι έχουμε κάποια εκ των προτέρων πληροφορία για την κατανομή της μέσης τιμής μ: \[p(μ)\sim N(μ_0,σ_0^{2})\]
\\Για το α ερώτημα, ξέρουμε ότι $μ_0=0$ και $σ_0^2=10σ^2$. Ακόμα ισχύει(τύποι σημειώσεων) ότι:\[p(μ|D) \sim N(μ_n,σ_n^2)\]
ή αλλιώς: \[p(μ|D) = \frac{1}{\sqrt{2π}σ_n}exp[-\frac{1}{2}((\frac{μ-μ_ν}{σ_n})^2)]\]
Με \[μ_n =\left( \middle \frac{nσ_0^2}{nσ_0^2+σ_^2}  \right)\bar{x} \]
και\[σ_n^2 = \frac{σ_0^2σ^2}{nσ_0^2+σ_^2}\]

Καθώς οι τιμές του n μεταβάλλονται από το 1 ως το 25, η γραφική της p(μ,H(n)) αλλάζει και αυτή όπως φαίνεται στο ακόλουθο σχήμα.\\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/5_a.png}
    \caption{p(μ,H(n)),για n=1:25}
    \label{fig:my_label}
\end{figure}
\\\\Στο ερώτημα Β, η πυκνότητα πιθανότητα ακολουθεί την εξής Gaussian κατανομή:\[p(x|D) \sim N(μ_n,σ^2+σ_n^2)\] 
Έτσι, προσαρμόζοντας τους τύπους για το $σ_n$ και το $μ_n$ , κρατώντας σταθερό το n=25, και αλλάζοντας την τιμή του $σ_0^2=10σ^2$ , $σ_0^2=σ^2$ , $σ_0^2=0.1σ^2$ , $σ_0^2=0.01σ^2$ προκύπει η ακόλουθη γραφική.\\\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/5_b.png}
    \caption{$σ_0^2=10σ^2$ , $σ_0^2=σ^2$ , $σ_0^2=0.1σ^2$ , $σ_0^2=0.01σ^2$}
    \label{fig:my_label}
\end{figure}
\\\\Παρατηρείται, ότι το πλάτος των Gaussian είναι ίδιο, και μάλιστα εξαρτάται από την τιμή του n. Από την άλλη, όσο μικραίνει η διασπορά τόσο η καμπύλη μας ολισθαίνει προς τα αριστερά. Αρχικά, πέφτοντας μια τάξη μεγέθους η ολίσθηση δεν είναι αισθητή, όσο όμως πέφτει και άλλο η τάξη μεγέθους, η καμπύλη ολισθαίνει όλο και περισσότερο.\\\\\\

\section*{Ερώτηση 7}
{\bfseries  Support Vector Machines :}\\ \\

Στην τελευταία άσκηση της παρούσας εργασίας, ασχοληθήκαμε με τον αλγόριθμο των support vector machines, και τον εφαρμόστηκε σε ένα σύνολο από τεχνητά δεδομένα. Πιο συγκεκριμένα, ορίστηκε ένα σύνολο n παραδειγμάτων \( \{(x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)})\}\), όπου \(x^{(i)} \in \Re^{1x2}\) και \(y^{(i)}\in\{-1,1\}\). 
\\\\Για να προβλέψουμε τις τιμές του y, από αυτές του x, θα χρησιμοποιηθεί η τεχνική svm. Για την υλοποιήση του svm, απαραίτητη προυπόθεση, είναι η εύρεση ενός υπερεπιπέδου, και των support vectors. Μοντελοποιώντας μαθηματικά το πρόβλημα, καταλήγουμε στο εξής: \[\hat{y} = \begin {cases}
                                                             1 & \tab,  \boldsymbol{wx^T+w_0 > 0} \\[0.3em]
                                                            -1&\tab,  \boldsymbol{wx^T+w_0 < 0}
                                                            \end{cases}\]
Οι παράμετροι του προβλήματος βρίσκονται με την λύση του προβλήματος βελτιστοποιήσης, κάτω από κάποιους περιορισμούς. Συγκεκριμένα. η συνάρτηση Lagrange δίνεται από την σχέση: \[\hat{L}(λ) = \displaystyle\sum_{i=1}^{n} λ^{(i)} - \frac{1}{2}\displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n} λ^{(i)}λ^{(j)}y^{(i)}y^{(j)}(x^{(i)}(x^{(j)})^T\]
Την οποία συνάρτηση την μεγιστοποιούμε κάτω από τους ΚΚΤ περιορισμούς: \[λ^{(i)}\geq0,\forall i \in[1,n]\]
\[\displaystyle\sum_{i=1}^{n} λ^{(i)}y^{(i)}=0\]
\\Όπως σε κάθε άσκηση, έτσι και σε αυτή, ξεκινάμε με την οπτικοποιήση του dataset.
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/svm11.png}
    \caption{The dataset}
    \label{fig:my_label}
\end{figure}
\\\\Για την εύρεση των πολλαπλασιαστών λ, το πρόβλημα ανάγεται σε πρόβλημα quadratic programming. Στην πραγματικότητα, η quadprog function του matlab, βρίσκει το ελάχιστο για το εξής πρόβλημα:\[min_{x}\frac{1}{2}x^TH_x+f^Tx,such that \begin{cases}   
A*x \le qb,\\
Aeq*x = beq,\\
lb\leq x\leq ub.
\end{cases}
\]
Με Η, να είναι ο πίνακας που περιγράφει την μορφή του hyperplane.\\
Σε ότι έχει να κάνει με την επιλογή των υπολοίπων μεταβλητών, όλες σχεδόν επιλέχτηκαν με βάση παραδείγματα που βρέθηκαν στον διαδίκτυο. Έξαίρεση αποτελεί το upper limit του λ, καθώς χωρίς τον περιορισμό από το C, δεν θα είχαμε τα επιθυμητά αποτελέσματα.\\\\
Με τον υπολογισμό των λ, είναι εφικτή η επιλογή των support vectors για το πρόβλημα. Στο σχήμα που ακολουθεί, έχουν "τονιστεί" τα στοιχεία εκείνα, από τα οποία θα διέρχονται τα sv, για τιμή C=10.
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/svm12.png}
    \caption{C=10}
    \label{fig:my_label}
\end{figure}
\\\ Τέλος, για να σχεδιαστούν σωστά τα support vectors, υπολογίζεται η max norma, με σκοπό τον υπολογισμό του width.\\\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/svm13.png}
    \caption{Support vectors and 2-D Hyperplane line,C=10}
    \label{fig:my_label}
\end{figure}
\\ Για το 2ο ερώτημα της άσκησης προστέθηκε ένα ακόμα δείγμα το οποίο ωστόσο με το μάτι δημιουργεί "πρόβλημα" στο classification.\\\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/svm21.png}
    \caption{51 samples dataset}
    \label{fig:my_label}
\end{figure}

\\ Σε αυτή την περίπτωση και αλλάζοντας την τιμή του C από 10 σε 100, και συμπεριλαμβάνοντας τον περιορισμό \(0 \leq λ^i \leq C^i \forall i\), καταλήγουμε στο ακόλουθο classification:\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/svm22.png}
    \caption{Support vectors and Hyperplane,C=100}
    \label{fig:my_label}
\end{figure}
\\Για το 3ο και τελευταιο μέρος της άσκησης, τα δεδομένα μας είναι μη γραμμικά διαχωρίσιμα, με αποτέλεσμα η άμεση εφαρμογή του svm αλγορίθμου να μην είναι δυνατή.\\\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=4cm]{images/svm31.png}
    \caption{Non linear separable dataset}
    \label{fig:my_label}
\end{figure}

\\\\Για να γίνει λοιπόν ο διαχωρισμός, έγινε προσθήκη ενός επιπλέον χαρακτηριστικού στα δεδομένα μας, και συγκερκιμένα προστέθηκε η norm-2 στο τετράγωνο του x.\\
Με την προσθήκη αυτή, και κρατώντας την μορφή του hypeplane, γραμμική πετυχαίνεται μια σχετικά καλή ταξινόμηση των δειγμάτων κάθε κλάσης.\\\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=6cm]{images/svm32.png}
    \caption{Classification of the dataset}
    \label{fig:my_label}
\end{figure}

\end{document}